Based on the tutorial at
https://www.tensorflow.org/tutorials/recurrent
.

time python tf-models/tutorials/rnn/ptb/ptb_word_lm.py \
  --data_path=ptb-data/ --model=small --save_path=ptb-model/small

=== On GPU

Running on a GeForce GTX 1080.  Takes 1m9s for test, 4m47s for small.
The speed figures printed are about 43k wps for test, 52k wps for small;
so for test, ~14x faster than brown-dwarf CPU.

Training the large model, `nvidia-smi` shows 95-98% GPU-Util, so we're
making good use of that, though only using 50% CPU (of one core).
That runs about 4.9k wps.

To make space for interactive desktop use, we should configure
`per_process_gpu_memory_fraction` to 0.9 or 0.8. See
https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth


=== On CPU

Requires --num_gpus=0 .

On CPU, with the generic wheel (not compiled for SSE etc.), even the
tiny test model (--model=test) takes 5m31s wall-clock on brown-dwarf,
17m06s total CPU time, to train.

... Then after all the work of building TF from source to get
`-march=native`, and even Intel's fancy MKL optimizations specifically
for TensorFlow, the resulting TF runs no faster.  Confirmed that when
installing the wheel into a fresh venv.

I guess I'm still doing better than this example in a book:
https://books.google.com/books?id=rsyqDQAAQBAJ&pg=PA149&lpg=PA149&dq=tensorflow+ptb+tutorial+speed&source=bl&ots=7O1R36T4um&sig=DHI9aONtKloo9YR85_lCuExIV2U&hl=en&sa=X&ved=0ahUKEwjav9Xa3KvWAhWIw1QKHT1jD20Q6AEISzAG#v=onepage&q=tensorflow%20ptb%20tutorial%20speed&f=false
400-450 wps.  I'm at 3000-3200.


Some discussion:
https://www.reddit.com/r/MachineLearning/comments/66rriz/d_rnns_are_much_faster_in_pytorch_than_tensorflow/
That's on GPU, so not directly comparable; and the slowdown is about 2.5x.

> Thanks! I think ptb_word_lm is meant more for tutorial purposes
  (illustrating how the problem works) and not as a good fast example
  for RNNs :(. I'll try to find time (or someone else) to get a
  version of that code running in a way that is more idiomatic to how
  we use it in practice, and will let you know when I do.

> Great! I do know if you use the lstm cudnn wrapper for TensorFlow it
  will go much faster. That is probably the best way to do things for
  right now.

but

> [... various issues ...] Overall cuDNN support for RNNs seems like
  an afterthought in TF.

